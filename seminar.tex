\documentclass[a4paper,twoside]{article}


\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{cmbright} %cmbright typeface
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{framed}
\usepackage[framed,thmmarks]{ntheorem}
\usepackage[normalem]{ulem}
\usepackage[pdfborder={1 1 0}]{hyperref}
\usepackage{fancyhdr}
\usepackage{todonotes}
\usepackage[ngerman]{babel}
\usepackage{makeidx}
\usepackage{biblatex}
\usepackage{graphicx}
\bibliography{seminar}
\setlength{\headheight}{12.5pt}

\fancypagestyle{mystyle}{ %
	\fancyhf{} % remove everything
	\renewcommand{\headrulewidth}{0.25pt} % remove lines as well
	\renewcommand{\footrulewidth}{0.25pt}
	\fancyhead[LE,RO]{\thepage}
	\fancyhead[RE,LO]{\leftmark}

}

	\addtolength{\headsep}{12pt}

%TODO spacing sollte genauso wie von section sein.
\makeatletter
\newtheoremstyle{newBreak}%
  {\item[\rlap{\vbox{\hbox{\hskip\labelsep \theorem@headerfont
          \large ##1\ ##2\theorem@separator}\hbox{\strut}}}]}%
  {\item[\rlap{\vbox{\hbox{\hskip\labelsep \theorem@headerfont
          \large ##1\ ##2\ ({##3})\theorem@separator}\hbox{\strut}}}]}
\makeatother
\theorembodyfont{}
\theoremstyle{newBreak}

\newcounter{thmc}[section] %theorem counter
\renewcommand{\thethmc}{\thesection.\arabic{thmc}}
\newframedtheorem{Definition}[thmc]{Definition}
\newtheorem{Bemerkung}[thmc]{Bemerkung}
\newtheorem{Beispiel}[thmc]{Beispiel}
\newframedtheorem{Exkurs}[thmc]{Exkurs}
\theoremsymbol{}
\newframedtheorem{Satz}[thmc]{Satz}
\newframedtheorem{Lemma}[thmc]{Lemma}
\newframedtheorem{Korollar}[thmc]{Korollar}

\theoremstyle{nonumberplain}

\theoremsymbol{\rule{1ex}{1ex}}
\newtheorem{Beweis}{Beweis}

\newcommand{\N}[0]{\mathbb{N}}
\newcommand{\Z}[0]{\mathbb{Z}}
\newcommand{\Q}[0]{\mathbb{Q}}
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\C}[0]{\mathbb{C}}
\newcommand{\G}[0]{\mathfrak{C}}
\newcommand{\E}[1][n]{\mathrm{E_{#1}}}
\newcommand{\Id}[1][n]{\mathrm{Id_{#1}}}

\newcommand{\plainset}[1]{\left\{#1\right\}}
\newcommand{\ouptoset}[1]{\plainset{1, ..., #1}} %one up to set
\newcommand{\zuptoset}[1]{\plainset{0, ..., #1}} %zero up to set
\newcommand{\condset}[2]{\left\{ #1: \; #2\right\}} %condition set

\renewcommand{\Re}[0]{\operatorname{Re}}
\renewcommand{\Im}[0]{\operatorname{Im}}
\newcommand{\sgn}[0]{\operatorname{Sgn}}
\newcommand{\argmin}[0]{\operatorname{Argmin}}
\newcommand{\argmax}[0]{\operatorname{Argmax}}
\newcommand{\dist}[0]{\operatorname{Dist}}
\newcommand{\rang}[0]{\operatorname{rang}}

\newenvironment{enumerateRef}{\begin{enumerate}[ref={\thethmc\alph*},label={(\alph*) }, leftmargin=*]}{\end{enumerate}}
\newenvironment{enumerateBew}{\mbox{}\begin{enumerate}[label={zu (\alph*):}, leftmargin=*]}{\end{enumerate}}
\makeindex

\begin{document}
	\title{Seminararbeit zur Kontrolle}
	\author{Elisa Friebel, Marcel Goesmann, Niklas Fischer}	
	\date{\today}
	\maketitle
	\begin{abstract}
		Diese von Michel Herty betreute Arbeit ist eine Ausarbeitung des Kapitels "`Finite-dimensional linear control 
		systems"' aus dem Buch "`Control and Nonlinearity"' von Jean-Michel Coron \cite{Coron2007}.
	\end{abstract}
	\tableofcontents
	\newpage
	\pagestyle{mystyle}


\setcounter{section}{-1}

\section{Einleitung}

\begin{center}
\begin{figure}[ht]
\includegraphics[clip,scale=0.7]{umlauf}\caption{Trajektorie der Dawn Sonde \cite{NASA2009}}
\end{figure}
\end{center}

In unserem Seminar wollen wir eine Einführung in die lineare Kontrolltheorie geben. Sie befasst sich hauptsächlich mit der Steuerung von Differentialgleichungen. Das heißt, die Trajektorie einer Lösung einer Differentialgleichung soll zu einer bestimmten Zeit $t_1$ einen bestimmten Zustand $x_1$ erreichen. Anwendung findet die Theorie zum Beispiel bei der Steuerung von Raumfahrzeugen, wie der Dawn Sonde. Ziel der Mission war es die Planeten Vesta und Ceres zu besuchen. Es war also eine Trajektorie gesucht, die von der Erde ($x_0$) im Zeitpunkt $t_0$ ausgehend Vesta ($x_1$) erreicht. 

\newpage

\section{Definitionen}
	\begin{Bemerkung}
		In der ganzen Arbeit gelten die folgenden Konventionen:
		\begin{enumerate}
			\item $T_0$ und $T_1$ seien reelle Zahlen und es sei $T_0 < T_1$.
			\item $A: [T_0, T_1] \rightarrow \R^{n \times n}$ und $B: [T_0, T_1] 
				\rightarrow \R^{n \times m}$ seien stetige Funktionen.
			\item $x: [T_0, T_1] \rightarrow \R^n$ ist eine stetige Funktion
		\end{enumerate}
	\end{Bemerkung}
	Betrachte zunächst folgende altbekannte Definition:
	\begin{Definition}[Anfangswertproblem]\label{AWP}\index{Anfangswertproblem}
	Sei $b: [T_0, T_1] \rightarrow \R^n$ eine stetige Funktion. Eine Funktion 
	$x$ wird Lösung des \emph{Anfangswertproblems (auch: AWP)} \[
		\dot x(t) = A(t) x(t) + b(t)
	\] genannt, falls diese Gleichung für alle $t \in [T_0, T_1]$ erfüllt ist, und
	für gegebene $x_0 \in \R^n$ und $t_0 \in [T_0, T_1]$ die Gleichung $x(t_0) = 
	x_0$ gilt.
	\end{Definition}
	Nun können lineare Kontrollsysteme beziehungsweise die Kontrollierbarkeit definiert werden, welche einen zentralen Punkt in der Arbeit einnehmen. 
	
	\begin{Definition}[lineares Kontrollsystem, kontrollierbar]\index{lineares Kontrollsystem}\index{kontrollierbar}
		Ein lineares Kontrollsystem 
			\[
				\dot x(t) = A(t)x(t) + B(t)u(t),\quad t \in [T_0, T_1]
			\]
		ist \emph{kontrollierbar}, falls es für alle $x_0, x_1 \in \R^n$ eine stetige Funktion $u$ gibt, sodass die Lösungsfunktion $x$ des Anfangswertproblems 
		\begin{align*}
			\dot x(t) = A(t)x(t) + B(t)u(t) \; \forall t \in [T_0, T_1] \quad \text{mit} \quad x(T_0) = x_0
		\end{align*}
		in $T_1$ den Wert $x_1$ annimmt.
	\end{Definition}
	Anschaulich kann man die Kontrollierbarkeit linearer Kontrollsysteme so interpretieren, dass zum 
	Beispiel ein Raumfahrzeug mit einer Steuerung $u$ vom Startpunkt $x_0$ zu 
	einem Punkt beliebigen anderen Punkt $x_1$ im Raum gebracht werden können muss. 
	Dabei modelliert $A$ die 
	"`natürlichen"' Kräfte auf das Objekt, und $B$ bildet Eigenschaften der 
	Steuerungseinheit des Raumschiffes, beispielsweise die Anordnung der 
	Triebwerke, ab. $u$ wiederum stellt die konkrete Steuerung dar, also zu 
	welchem Zeitpunkt die Triebwerke wie angesteuert werden sollen. 
	In der Raumfahrt geht es darum eine Raumsonde von der Erde aus zu einem einzigen bestimmten Punkt zu steuern.
	Im Gegensatz dazu ist es für die Kontrollierbarkeit eines Systems wichtig, 
	dass die Sonde von jedem Punkt ausgehend jeden anderen Erreichen kann.
	
	
	\section{Die Resolvente}
	\subsection*{Einleitung}
		Von besonderem Interesse ist jetzt natürlich ein Kriterium für die 
		Kontrolleribarkeit eines linearen Kontrollsystems. Dieses wird in
		Abschnitt \ref{Gram'sche Kontrollmatrix} geliefert. Hierfür ist allerdings
		etwas Vorarbeit notwendig.
		
		Für lineare Differentialgleichungssysteme \[
			\dot x(t) = A \cdot x(t)
		\] mit konstanten $A \in \R ^ {n \times n}$ ist es mithilfe der 
		Exponentialfunktion einfach ein Fundamentalsystem zu bestimmen, doch was
		geschieht, wenn $A$ wie in der in \ref{AWP} gegebenen Definition eines 
		Anfangswertproblems abhängig von $t$ ist? In diesem Fall ist die
		Fundamentalmatrix im allgemeinen nicht elementar bestimmbar.
		
		Diese Klippe wird mit Resolvente $R$, die uns eine eindeutig bestimmte 
		Fundamentalmatrix eines Anfangswertproblems	liefert, geschickt umschifft.
		Mit der Resolvente und dem zentralen Satz aus Abschnitt
		\ref{Gram'sche Kontrollmatrix} können kann dann überpüft werden, ob das 
		lineare Kontrollproblem kontrollierbar ist. 
	\subsection*{Definition}
		Sei $\Phi:\R \rightarrow \R^{n \times n}$ die Funktion, die das homogene Anfangswertproblem
		\[
			\dot M(t) = A(t)\cdot M(t) \; \forall t \in [T_0, T_1]  \quad \text{mit} \quad M(\tau) = \E
		\]
		löst. Hierbei steht $\E \in \R^{n \times n}$ für die Einheitsmatrix. Offensichtlich ist $\Phi$ von der Wahl von $\tau$ abhängig. Definiere
		$\tilde R$ als die Funktion, die $\tau$ auf die Funktion $\Phi$ 
		abbildet:
		\[
			\tilde R : [T_0, T_1] \rightarrow C^0( [T_0, T_1],\R^{n \times n}), 
			\tau \mapsto \Phi
		\]
		Jetzt kann die Resolvente einfach definiert werden:
		
		\begin{Definition}[Resolvente]\index{Resolvente}
			Die Funktion 
			\[
				R: [T_0, T_1] \times [T_0, T_1] \rightarrow \R^{n \times n},
				(t, \tau) \mapsto \tilde R(\tau)(t)
			\]
			heißt Resolvente.
		\end{Definition}

		\begin{Beispiel}
			Gegeben sei die Differentialgleichung 
			\begin{align*}
				\dot M(t) = A(t) \cdot M(T) \quad \text{mit} \quad A(t) :=
				\begin{pmatrix}
					t & -1 \\
					1 & t
				\end{pmatrix}
				\label{Resolvente:Beispiel:star}\tag{$\star$}
			\end{align*} von der wir die Resolvente bestimmen wollen.
			Die Differentialgleichung \eqref{Resolvente:Beispiel:star} kann mit
			\begin{align*}
				& z(t) = x_1(t) + i x_2(t)
				\label{Resolvente:Beispiel:star2}\tag{$\star\star$} \\
				\Rightarrow & \dot z(t) = \dot x_1(t) + i \dot x_2(t)
			\end{align*} umgeschrieben werden:
			\begin{align*}
				\dot z(t) =  & ( t x_1(t) -x_2(t))+ i ( x_1(t) +t x_2(t)) \\
				= & t ( x_1(t) + i (x_2) ) + i (x_1(t) + i x_2(t)) \\
				= & t \cdot z(t) + i \cdot z(t)\\
			 	= & (t + i) \cdot z(t)
			\end{align*}
			Betrachte also das Anfangswertproblem
			\begin{align*}
				\dot z(t) = (t + i)  \cdot z(t) \quad \text{mit} \quad z(t_2) = 1
			\end{align*}
			welches mit dem Korollar über lineare Differentialgleichungen aus \cite{KriegWalcher2010}[S. 14]
			\[
				z(t) = z(t_2) \exp \left( \frac{t^2}2 - \frac{t_2^2}2 + i t - i t_2 \right)
			\]
			ergibt. Mit \eqref{Resolvente:Beispiel:star2} folgt auch \[
				x_1(t) = \Re( z(t)) \quad \text{und} \quad x_2(t) = \Im(z(t)).
			\]
			Dann ist die Resolvente gegeben durch 
			\[
				R(t, t_2) =
				\begin{pmatrix}
					\cos(t-t_2) \exp \left( \frac{t^2}2 - \frac{t_2^2}2 \right) &
					-\sin(t-t_2) \exp \left( \frac{t^2}2 - \frac{t_2^2}2 \right) \\
					\sin(t-t_2) \exp \left( \frac{t^2}2 - \frac{t_2^2}2 \right) &
					\phantom{-}\cos(t-t_2) \exp \left( \frac{t^2}2 - \frac{t_2^2}2 \right)
				\end{pmatrix}
			\]
			
		\end{Beispiel}
	\subsection*{Eigenschaften}
		\begin{Lemma}
			$R$ ist stetig.
		\end{Lemma}
		\begin{Beweis}
			Die Stetigkeit von $R$ in der 1. Komponente ($t$) folgt aus der Stetigkeit
			jeder Lösungsfunktion $\Phi$ des von $\tau$ abhängigen Anfangswertproblems.
		
			Die Stetigkeit in der 2. Komponente ($\tau$) folgt aus dem Satz über die
			Stetige Abhängigkeit aus \cite{KriegWalcher2010}.
			Für $\tau_1, \tau_2 \in [T_0,T_1]$ betrachte die Anfangswertprobleme
			\[
				\dot M(t) = A(t)\cdot M(t) \; \forall t \in [T_0, T_1]  \quad \text{mit} \quad M(\tau_1) = \E
			\]
			und
			\[
				\dot M(t) = A(t)\cdot M(t) \; \forall t \in [T_0, T_1]  \quad \text{mit} \quad M(\tau_2) = \E
			\]
			mit Lösungen $\Phi_1$ bzw. $\Phi_2$.
			Dann existiert zu jedem $\varepsilon > 0 $ ein $\delta := | \tau_1 - \tau_2 |$
			sodass \[
				|| \Phi_1(t) - \Phi_2(t) || < \varepsilon \; \forall t \in [T_0, T_1]
			\]
			dies ist äquivalent zur Stetigkeit von $R$ in der 2. Komponente.
			Da $R$ in beiden seiner Komponenten stetig ist, mit $R$ stetig.
		\end{Beweis}
		
		\begin{Lemma}\label{Resolvente Eigenschaften}
			Die Resolvente erfüllt die folgenden Eigenschaften
			\begin{enumerateRef}
				\item\label{Resolvente Eigenschaften:1} $	R(t_1,t_1) = \E$
				\item\label{Resolvente Eigenschaften:2} $ R(t_1, t_2) \cdot R(t_2, t_3) = R(t_1, t_3)$
				\item\label{Resolvente Eigenschaften:3} $ R(t_1, t_2) \cdot R(t_2, t_1) = \E$
			\end{enumerateRef}
			für alle $t_1, t_2, t_3 \in [T_0, T_1]$.
		\end{Lemma}
		
		\begin{Beweis}
		\begin{enumerateBew}
			\item Betrachten wir das zu $R(t_1, t_1) $ gehörige 
			Anfangswertproblem \[
				\dot M(t) = A(t) \cdot M(t) \; \forall t \in [T_0, T_1] \quad \text{mit} \quad M(t_1) = \E
			\] mit Lösung $\Phi$.
			Nun gilt: \[
				R(t_1, t_1) = \tilde R (t_1)(t_1) = \Phi(t_1) = \E.
			\]
			\item 
				Betrachte die wie in \cite{KriegWalcher2010}[S. 43] beschriebene Vektorraumeigenschaft von homogenen Differentialgleichungen. Interessant ist, dass die Abbildung die einem Anfangswert eine Lösungsfunktion zuordnet ein Isomorphismus ist. Wenn also die Funktion $\Phi$ das Anfangswertproblem \[
					\dot M(t) = A(t) M(t) \quad \text{mit} \quad M(t_0) = C
				\] löst,
				so löst $\Phi \cdot D$ das Anfangswertproblem \[
					\dot M(t) = A(t) M(t) \quad \text{mit} \quad M(t_0) = C \cdot D.
				\]

				Mit dieser Eigenschaft kann die Aussage gezeigt werden. Betrachte $\tilde R(t_2)$ und $\tilde R(t_3)$. Diese beiden Funktionen sind die Lösungen der Anfangswertprobleme 
				\[
					\dot M_2(t) = A(t) M_2(t) \quad \text{mit} \quad M_2(t_2) = \E.
				\]				
				beziehungsweise
				\[
					\dot M_3(t) = A(t) M_3(t) \quad \text{mit} \quad M_3(t_3) = \E
				\]
				Also löst $\tilde R(t_2) \cdot R(t_2,t_3)$ das Anfangswertproblem
				\[
					\dot M_2(t) = A(t)M_2(t) \quad \text{mit} \quad M_2(t_2) = R(t_2,t_3).
				\]
				Wertet man nun $\tilde R(t_2) \cdot R(t_2,t_3)$ in $t_2$ aus, so erhält man
				\[
					R(t_2,t_2) \cdot R(t_2,t_3) = R(t_2,t_3) = \tilde R(t_3)(t_2).
				\]
				Da die Funktion $(t,x) \mapsto A(t) x$ in $x$ lokal einer Lipschitzbedingung genügt, sind die Lösungen von Anfangswertproblemen eindeutig bestimmt. Aus der obigen Gleichung folgt also, dass die Bahnen von $\tilde R(t_3)$ und $\tilde R(t_2) \cdot R(t_2,t_3)$ in $t_2$ übereinstimmen. Daraus folgt, dass beide Bahnen identisch sein müssen, was dazu führt, dass die Lösungsfunktionen in $t_1$ ausgewertet auch übereinstimmen müssen.
			\item 
				\[
					R(t_1, t_2) \cdot R(t_2, t_1) \overset{\ref{Resolvente Eigenschaften:2}}= R(t_1, t_1) \overset{\ref{Resolvente Eigenschaften:1}}= E_n
				\]
		\end{enumerateBew}
		\end{Beweis}
		
		\begin{Lemma}\label{Resolvente Ableitung}
			Für die Ableitungen der Resolvente gilt:
			\begin{enumerateRef}
				\item\label{Resolvente Ableitung:1} $ \frac{\partial R}{\partial t} (t, \tau) = A(t) \cdot R(t,\tau) \; \forall t,\tau \in [T_0, T_1]$
				\item\label{Resolvente Ableitung:2} $ \frac{\partial R}{\partial \tau} (t, \tau) = - R(t, \tau) \cdot A(\tau) \; \forall t,\tau \in [T_0, T_1]$
			\end{enumerateRef}
		\end{Lemma}
		
		\begin{Beweis}
			\begin{enumerateBew}
				\item
					Die Definition der Resolvente mit ihrem Anfangswertproblem liefert uns
					die Interpretation \[
						R(t, \tau ) = E_n + \int_\tau^t A(s) \cdot R(s, \tau) ds.
					\]
					Differenzieren liefert
					\[
						\frac{\partial R}{\partial t}R(t,\tau) = A(t) \cdot R(t, \tau).
					\]
				\item
					Differenzieren von \ref{Resolvente Eigenschaften:3} nach $t_2$ gibt uns
					\[
						\frac{\partial R}{\partial t_2} E_n = \frac{\partial R}{\partial t_2} R(t_1, t_2) \cdot R(t_2, t_1)
					\] also \[
						0 = \left[ \frac{\partial R}{\partial t_2} R(t_1, t_2) \right] \cdot R(t_2, t_1) +
							R(t_1, t_2) \cdot \underbrace{\left[ \frac{\partial R}{\partial t_2}R(t_2, t_1) \right]}_{=A(t_2) \cdot R(t_2, t_1)} 
					\] woraus \[
						0 = \left [ \frac{\partial R}{\partial t_1}R(t_1, t_2) + R(t_1, t_2) \cdot A(t_2) \right] \cdot R(t_2, t_1)
					\] folgt.
					
					\ref{Resolvente Eigenschaften:3} liefert, dass $R(t_2, t_1)$ 
					invertierbar ist ($R(t_1, t_2)$ ist die zugehörige Inverse). Also
					gilt:
					\[
						\frac{\partial R}{\partial t_2}R(t_1, t_2) = - R(t_1, t_2) \cdot A(t_2),
					\]
					was zu zeigen war.
			\end{enumerateBew}
		\end{Beweis}

		\begin{Lemma}\label{Resolvente AWP}
			Für die Lösung $x$ des Anfangswertproblems \begin{align*}
				\dot x(t) = A(t) x(t) + b(t) \; \forall t \in [T_0, T_1] \quad \text{mit} \quad x(T_0) = x^0
				\label{Resolvente AWP:star}\tag{$\star$}
			\end{align*} gilt 
			\begin{enumerateRef}
				\item\label{Resolvente AWP:1} $x(t_1) = R(t_1, t_0) x(t_0) + \int_{t_0}^{t_1} R(t_1,\tau) b(\tau) d\tau$
				\item\label{Resolvente AWP:2} $x(t) = R(t, T_0) x^0+ \int_{T_0}^t R(t, \tau)  b(\tau)d\tau$.
			\end{enumerateRef} für alle $t_0, t_1, t \in [T_0, T_1]$
		\end{Lemma}
		\begin{Beweis}
			\begin{enumerateBew}
				\item
					 Differenzieren von \ref{Resolvente AWP:1} nach $t_1$ ergibt 								\begin{align*}
						& \frac \partial{\partial t_1} x(t_1) \\
						 = & \frac \partial{\partial t_1} \left(R(t_1, t_0)x(t_0) + \int_{t_0}^{t_1} R(t_1,\tau) b(\tau) d\tau \right)\\
						 = & \frac \partial {\partial t_1}  R(t_1,t_0)x(t_0) + \frac \partial {\partial t_1} \int_{t_0}^{t_1} R(t_1,\tau) b(\tau) d\tau.
						%LEIBNITZ REGEL FÜR PARAMETERINTEGRALE
						\intertext{Mit der Leibnitzregel für Parameterintegrale folgt}
						= & A(t_1)R(t_1,t_0)x(t_0) + \\ & \qquad \int_{t_0}^{t_1} \frac \partial {\partial t_1} R(t_1,\tau)b(\tau)d\tau +  R(t_1,t_1)b(t_0) \cdot 1 - R(t_1,t_0)b(t_0) \cdot 0 \\
						= & A(t_1)R(t_1,t_0)x(t_0) + \int_{t_0}^{t_1} A(t_1) R(t_1,\tau)b(\tau)d\tau +  R(t_1,t_1)b(t_1) \\
						= & A(t_1)\left[R(t_1,t_0)x(t_0) + \int_{t_0}^{t_1} R(t_1,\tau)b(\tau )d\tau \right] + b(t_1) \\
						= & A(t_1) x(t_1) + b(t_1).
					\end{align*}
					Also erfüllt \ref{Resolvente AWP:1} das Anfangswertproblem \eqref{Resolvente AWP:star}.
				\item
					Spezialfall von \ref{Resolvente AWP:1} mit $t_0:= T_0$ und $t_1 := t$ 
			\end{enumerateBew}
		\end{Beweis}
		
	\section{Gram'sche Kontrollmatrix}\label{Gram'sche Kontrollmatrix}
		\begin{Definition}[Gram'sche Kontrollmatrix]\index{Gram'sche Kontrollmatrix}
			Für ein Kontrollsystem \[
				\dot x(t) = A(t) x(t) + B(t) u(t)
			\]
			heißt die Matrix \[
				\G:=\int_{T_0}^{T_1} R(T_1, \tau) \cdot B(\tau) \cdot B(\tau)^T\cdot R(T_1, \tau)^T d\tau \in \R^{n \times n}
			\] Gram'sche Kontrollmatrix
		\end{Definition}
		
		\begin{Lemma}\label{Gram'sche Kontrollmatrix:definitheit}
			\begin{enumerateRef}
				\item $\G$ ist symmetrisch\label{Gram'sche Kontrollmatrix:Eigenschaften:symmetrie}
				\item $\G$ ist positiv semidefinit\label{Gram'sche Kontrollmatrix:Eigenschaften:definitheit}
			\end{enumerateRef}
		\end{Lemma}
		
		\begin{Beweis}
			\begin{enumerateBew}
			\item Für eine beliebige quadratische Matrix $M = (m)_{i,j}\in \R^{n \times n}$ ist $M\cdot M^T$
			symmetrisch. Da das Integral einer Matrix als Integral über die Matrixeinträge 
			definiert ist, und $m_{i,j} = m_{j,i}$ gilt, ist das Integral über eine symmetrische
			Matrix wieder symmetrisch. Wähle nun $M := R(T_1,\tau)\cdot B(\tau)$, dann ist
			$\G$ symmetrisch.
			\item Es gilt für $x \in \R^n$:
				\begin{align*}
				x^T \G x = & x^T \int_{T_0}^{T_1} R(T_1, \tau) B(\tau) B(\tau)^T R(T_1, \tau)^T d\tau \; x\\
					=& \int_{T_0}^{T_1} \langle B(\tau)^T R(T_1, \tau)^T x, B(\tau)^T R(T_1, \tau)^T x\rangle d\tau \\
					=& \int_{T_0}^{T_1} | B(\tau)^T R(T_1, \tau)^T x|^2 d\tau \geq 0.
			\end{align*}
			Damit ist $\G$ positiv semidefinit.
		\end{enumerateBew}
		\end{Beweis}
		
		\begin{Satz}\label{Kontrollierbarkeit}
			Ein lineares Kontrollsystem \[
				\dot x(t) = A(t) \cdot x(t) + B(t) \cdot u(t)
			\] ist genau dann kontrollierbar, wenn seine
			Gram'sche Kontrollmatrix regulär ist.
		\end{Satz}
		
		\begin{Beweis}
			\begin{enumerate}
				\item["`$\Leftarrow$"': ]
					Sei $\G$ invertierbar, $x^0, x^1 \in \R^n$ beiliebig. Definiere \[
						\bar u: (T_0, T_1) \rightarrow \R^m, \tau \mapsto B(\tau)^T\cdot R(T_1,\tau)^T\G^{-1}\cdot (x^1-R(T_1, T_0)\cdot x^0)
					\]
					Dann ist $\bar u$ stetig, da $\G^{-1}(x^1-R(T_1, T_0)\cdot x^0)$ eine
					von $\tau$ unabhängige Konstante ist, und $B$, sowie $R$ stetig sind.
					
					Sei $\bar x$ die Lösung des Anfangswertproblems \[
						\dot {\bar x}(t) = A(t) \cdot \bar x(t) + B(t) \cdot \bar u (t) \quad \text{mit} \quad \bar x (T_0) = x^0
					\]
					
					Nach \ref{Resolvente AWP:2} gilt: \begin{align*}
						\bar x(T_1) = & R(T_1, T_0) x^0 + \int_{T_0}^{T_1} R(T_1, \tau )  B(\tau) \bar u(\tau) d\tau \\
						 = & R(T_1, T_0) x^0 + \\
						 & \qquad \int_{T_0}^{T_1} R(T_1, \tau ) B(\tau) B(\tau)^T R(T_1,\tau)^T\G^{-1}(x^1-R(T_1, T_0)x^0) d\tau\\
						 = & R(T_1, T_0) x^0 + \\
						 & \qquad \underbrace{\int_{T_0}^{T_1} R(T_1, \tau ) B(\tau) B(\tau)^T R(T_1,\tau)^T d\tau}_{=\G} \; \G^{-1}(x^1-R(T_1, T_0)x^0) \\
						 = & R(T_1, T_0) x^0 + x^1-R(T_1, T_0)x^0 \\
						 = & x^1
					\end{align*}
					
					also gibt es eine Steuerung $\bar u$, die die gewünschten Bedingungen erfüllt.
					
				\item["`$\Rightarrow$"': ]
					Falls $\G$ singulär ist, gibt es ein $y \in \R^n\setminus \plainset{0}$ mit \[
						\G y = 0.
					\]
					Also gilt auch
					\begin{align*}
						0 = y^T \G y = &\int_{T_0}^{T_1} y^T R(T_1,y)B(\tau)B(\tau)^T R(T_1, \tau)^T y d\tau \\
						= & \int_{T_0}^{T_1} | B(\tau)^T R(T_1, \tau)^T y |^2 d\tau.
					\end{align*}
					Daraus folgt 
					\begin{align*}
						y^T R(T_1, \tau) B(\tau) = 0 \tag{$\star$}\label{eqn:Gramsche Kontrollmatrix:star}
					\end{align*}
					da $\tau \mapsto B(\tau)^T R(T_1, \tau)^T y$ stetig ist.
					Sei nun $u: [T_0, T_1] \rightarrow R^m$ eine beliebige stetige Funktion. Dazu sei $x$ die Lösung des Anfangswertproblems \[
						\dot x(t) = A(t) x(t) + B(t) u(t) \quad \text{mit} \quad x(T_1) = 0 = x^1.
					\]
					Dann gilt wieder mit \ref{Resolvente AWP:2} \begin{align*}
						x(t) = & R(t, T_0) \cdot x^0+ \int_{T_0}^t R(t, \tau) \cdot B(\tau) u(\tau) d\tau \\
								 \overset{x^0=0}= & \int_{T_0}^t R(t, \tau) \cdot B(\tau) u(\tau) d\tau.
								 \tag{$\star\star$}\label{eqn:Gramsche Kontrollmatrix:star2}
					\end{align*}
					
					Aus \eqref{eqn:Gramsche Kontrollmatrix:star} und \eqref{eqn:Gramsche Kontrollmatrix:star2} folgt
					\begin{align*}
						y^T x(t) = & y^T \int_{T_0}^t R(t, \tau) \cdot B(\tau) u(\tau) d\tau \\
						= & \int_{T_0}^t \underbrace{ y^T R(t, \tau) \cdot B(\tau)}_{=0} u(\tau) d\tau = 0
					\end{align*}
					also, dass $ y^T x(T_1) = 0$ für beliebige $u$ gilt.
					
					Es existiert aber ein $x^1 \in \R^n$ mit $y^T x^1 \neq 0$, beispielsweise $x^1 = y$.
					
					Wenn das Kontrollproblem kontrollierbar wäre würde es eine Steuerung $u$ geben, sodass $x(T_1) = x^1$.
					Daraus würde dann aber auch \[0 = y^T x(T_1) = y^T x^1 \neq 0\] folgen, was ein Widerspruch ist.
					
					Also kann das Kontrollproblem nicht kontrollierbar sein, wenn $\G$ singulär ist.
			\end{enumerate}
		\end{Beweis}
		Mit $\bar u$ ist jetzt also eine allgemeine Steuerung gefunden. Aber $\bar u$ 
		erfüllt auch noch eine Minimalitätsbedingung:
		
		\begin{Satz}\label{Gramsche Minimalitaet}
			Seien $T_0, T_1\in \R$ sowie $x^0, x^1 \in \R^n$. Für ein beliebiges Kontrollproblem
			\[
				\dot x = A(t) x(t) + B(t) u(t) \; \forall t\in [T_0, T_1] \quad \text{mit} \quad x(T_0)=x^0, \; x(T_1)=x^1
			\]
			mit Lösung $u$ gilt
			\begin{enumerateRef}
				\item $\int_{T_0}^{T_1} |\bar u(\tau)|^2 d\tau < \int_{T_0}^{T_1} |u(\tau)|^2d\tau$, falls $u \neq \bar u$ und
				\item $\int_{T_0}^{T_1} |\bar u(\tau)|^2 d\tau = \int_{T_0}^{T_1} |u(\tau)|^2d\tau$, falls $u = \bar u$
			\end{enumerateRef}
			wobei
			\[
				\bar u: (T_0, T_1) \rightarrow R^m, \tau \mapsto B(\tau)^T\cdot R(T_1,\tau)^T\G^{-1}\cdot (x^1-R(T_1, T_0)\cdot x^0)
			\]
		\end{Satz}
		
		\begin{Beweis}
			Definiere \[
				v := u - \bar u.
			\]
			Betrachte zunächst $|u(t)|^2$
			\begin{align*}
				|u(t)|^2 = & |\bar u(t) + v(t)|^2 \\
					= & \langle \bar u(t) + v(t), \bar u(t) + v(t)\rangle \\
					= & \langle \bar u(t), \bar u(t)\rangle + 2 \langle \bar u(t), v(t)\rangle + \langle v(t),v(t) \rangle \\
					= & |\bar u(t) |^2 + 2 \bar u(t)^T v(t) + |v(t)|^2
			\end{align*}
			Also gilt auch
			\[
				\int_{T_0}^{T_1}|u(\tau)d\tau|^2 = \int_{T_0}^{T_1}|\bar u(\tau) |^2 + 2 \int_{T_0}^{T_1}\bar u(\tau)^T v(\tau) d\tau+ \int_{T_0}^{T_1}|v(\tau)|^2 d\tau.
			\]
		Für $\int_{T_0}^{T_1}\bar u(\tau)^T v(\tau) d\tau$ aber gilt
		\begin{align*}
			& \int_{T_0}^{T_1}\bar u(\tau)^T v(\tau) d\tau \\
			\overset{\text{Def. }u}= & \int_{T_0}^{T_1}(B(\tau)^T R(T_1,\tau)^T\G^{-1} (x^1-R(T_1, T_0) x^0))^T v(\tau) d\tau \\
			\overset{\G^{-1} \text{ sym.}}= & \int_{T_0}^{T_1} (x^1-R(T_1,T_0)x^0)^T \G^{-1}R(T_1,\tau)B(\tau)v(\tau)d\tau \\
			= \;\;\,& (x^1-R(T_1,T_0)x^0)^T \G^{-1} \int_{T_0}^{T_1} R(T_1,\tau)B(\tau)v(\tau)d\tau
		\end{align*}
		und für $\int_{T_0}^{T_1} R(T_1,\tau)B(\tau)v(\tau)d\tau$ gilt 
		\begin{align*}
			 & \int_{T_0}^{T_1} R(T_1,\tau)B(\tau)v(\tau)d\tau \\
			=& \int_{T_0}^{T_1} R(T_1,\tau)B(\tau)u(\tau)d\tau - \int_{T_0}^{T_1} R(T_1,\tau)B(\tau)\bar u(\tau)d\tau \\
			\overset{\eqref{eqn:Gramsche Minimalitaet:star}}=& (x(T_1) - R(T_1,T_0)x(T_0)) - (\bar x(T_1)-R(T_1,T_0) \bar x(T_0)) \\
			\overset{x, \bar x \text{ Lsg.}}=& (x^1-R(T_1,T_0)x^0) - (x^1-R(T_1,T_0)x^0) \\
			=& 0
			\tag{$\star\star$}\label{eqn:Gramsche Minimalitaet:star2} 
		\end{align*}
		wobei \eqref{eqn:Gramsche Minimalitaet:star} aus Umstellen von \ref{Resolvente AWP:2} folgt:
		\begin{align*}
			  & x(T_1) =R(T_1,T_0) x^0 + \int^{T_1}_{T_0}R(T_1,\tau)B(\tau)u(\tau)d\tau \\
			\Leftrightarrow {}&  x(T_1) - R(T_1,T_0) x^0 = \int^{T_1}_{T_0}R(T_1,\tau)B(\tau)u(\tau)d\tau
			\tag{$\star$}\label{eqn:Gramsche Minimalitaet:star} .
		\end{align*}
		Mit \eqref{eqn:Gramsche Minimalitaet:star2} gilt also
		\[
			\int_{T_0}^{T_1} R(T_1,\tau)B(\tau)v(\tau)d\tau = 0
		\]
		und damit
		\[
			\int_{T_0}^{T_1}|u(\tau)d\tau|^2 = \int_{T_0}^{T_1}|\bar u(\tau) |^2 +  \underbrace{\int_{T_0}^{T_1}|v(\tau)|^2 d\tau.}_{\geq 0}
		\]
		\end{Beweis}
		$\bar u$ ist also diejenige Steuerung, sodass $\int_{T_0}^{T_1} |u(\tau)|^2d\tau$ minimal ist.
		Anschaulich ist der Vorteil dieser Bedingung auch sofort klar. Wenn zum 
		Beispiel wie anfangs ein Raumfahrzeug gesteuert werden sollte, so sorgt diese
		Anforderung dafür, dass möglichst wenig Energie für die Steuerung verbraucht
		wird.
		
\input{kalman}
\input{marcel}
\newpage
\printbibliography
\newpage
\printindex
\end{document}

% ===REFERENZEN===
% Krieg - Gew. DGl
% Rannacher
